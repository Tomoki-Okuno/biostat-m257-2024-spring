{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Biostat/Biomath M257 Homework 6\n",
    "\n",
    "Due June 7 @ 11:59PM\n",
    "\n",
    "Tomoki Okuno and 805851067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System information (for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.10.0\n",
      "Commit 3120989f39b (2023-12-25 18:01 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (arm64-apple-darwin22.4.0)\n",
      "  CPU: 8 × Apple M1\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\n",
      "  Threads: 2 on 4 virtual cores\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/07_UCLA/Class/257/02_Homework/hw6`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Documents/07_UCLA/Class/257/02_Homework/hw6/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.5.0\n",
      "  \u001b[90m[336ed68f] \u001b[39mCSV v0.10.14\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.6.1\n",
      "  \u001b[90m[8bb1440f] \u001b[39mDelimitedFiles v1.9.1\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.108\n",
      "  \u001b[90m[b6b21f68] \u001b[39mIpopt v1.6.2\n",
      "  \u001b[90m[67920dd8] \u001b[39mKNITRO v0.14.2\n",
      "  \u001b[90m[b8f27783] \u001b[39mMathOptInterface v1.30.0\n",
      "  \u001b[90m[ff71e718] \u001b[39mMixedModels v4.24.0\n",
      "  \u001b[90m[76087f3c] \u001b[39mNLopt v1.0.2\n",
      "  \u001b[90m[08abe8d2] \u001b[39mPrettyTables v2.3.2\n",
      "  \u001b[90m[6f49c342] \u001b[39mRCall v0.14.1\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we continue with the linear mixed effects model (LMM) considered in HW3\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma}_i + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma}_i \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma_0^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T.\n",
    "$$\n",
    "Because the variance component parameter $\\boldsymbol{\\Sigma}$ has to be positive semidefinite, we prefer to use its Cholesky factor $\\mathbf{L}$ as optimization variable. \n",
    "\n",
    "Given $m$ independent data tuples $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2).\n",
    "$$\n",
    "In this assignment, we use the nonlinear programming (NLP) approach for optimization. In HW7, we will derive an EM (expectation-maximization) algorithm for the same problem. There is also an MM (minorization-maximization) algorithm for the same problem; see [this article](https://doi.org/10.1080/10618600.2018.1529601)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathOptInterface"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using Ipopt, LinearAlgebra, MathOptInterface, MixedModels, NLopt\n",
    "using PrettyTables, Random, RCall\n",
    "\n",
    "const MOI = MathOptInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (Optional, 30 bonus pts) Derivatives\n",
    "\n",
    "NLP optimization solvers expect users to provide at least a function for evaluating objective value. If users can provide further information such as gradient and Hessian, the NLP solvers will be more stable and converge faster. Automatic differentiation tools are becoming more powerful but cannot apply to all problems yet.\n",
    "\n",
    "1. Show that the gradient of $\\ell_i$ is\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &= \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i, \\\\\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &= - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i, \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &= - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L},\n",
    "\\end{aligned}\n",
    "where $\\mathbf{r}_i = \\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}$. \n",
    "\n",
    "2. Derive the observed information matrix and the expected (Fisher) information matrix.\n",
    "\n",
    "If you need a refresher on multivariate calculus, my [Biostat 216 lecture notes](https://ucla-biostat216-2019fall.github.io/slides/16-matrixcalc/16-matrixcalc.html) may be helpful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Use the chain rule in 1.2 of 216 lecture notes basically. Note that $D\\phi(\\mathbf{x}) = \\nabla\\phi(\\mathbf{x})^{\\color{red}T}$ when $\\phi$ is a scalar function and $\\mathbf{x}$ is a vector.\n",
    "\n",
    "1. Gradient w.r.t $\\boldsymbol{\\beta}$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= -\\frac{1}{2}\\nabla_{\\boldsymbol{\\beta}}\\mathbf{r}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\\\\n",
    "&= -\\frac{1}{2}\\left(D_{\\mathbf{r}_i}\\mathbf{r}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\cdot D_{\\boldsymbol{\\beta}}(\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\right)^T\\\\\n",
    "&= -\\frac{1}{2}\\left({2\\mathbf{r}_i}^T\\boldsymbol{\\Omega}_i^{-1} \\cdot (-\\mathbf{X}_i) \\right)^T\n",
    "\\quad\\because D_{\\boldsymbol{\\beta}}(\\mathbf{X}_i \\boldsymbol{\\beta}) = \\mathbf{X}_i\\\\\n",
    "&= \\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Gradient w.r.t $\\sigma^2$. Using the spectral decomposition $\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T = \\mathbf{T}\\boldsymbol{\\Lambda}\\mathbf{T}^T$,\n",
    "where $\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_{n_i})$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\sigma^2}\\log\\det\\boldsymbol{\\Omega}_i\n",
    "&= \\nabla_{\\sigma^2}\\log\\det(\\mathbf{T}(\\sigma^2\\mathbf{I}_{n_i} + \\boldsymbol{\\Lambda})\\mathbf{T}^T)\\\\\n",
    "&= \\nabla_{\\sigma^2}\\sum_{j=1}^{n_i}\\log(\\sigma^2 + \\lambda_j)\n",
    "\\quad\\because\\det(\\mathbf{T}\\mathbf{T}^T) = 1\\\\\n",
    "&= \\sum_{j=1}^{n_i}(\\sigma^2 + \\lambda_j)^{-1}\\\\\n",
    "&= \\text{tr}(\\mathbf{T}(\\sigma^2\\mathbf{I}_{n_i} + \\boldsymbol{\\Lambda})^{-1}\\mathbf{T}^T)\n",
    "\\quad\\because\\mathbf{T}^T\\mathbf{T} = \\mathbf{I}_{n_i}\\\\\n",
    "&= \\text{tr}\\left(\\boldsymbol{\\Omega}_i^{-1}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Using (59) in the [Matrix Cook Book](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf):\n",
    "$$\n",
    "\\frac{\\partial\\mathbf {Y}^{-1}}{\\partial x} = -\\mathbf{Y}^{-1}\\frac{\\partial\\mathbf {Y}}{\\partial x}\\mathbf{Y}^{-1}\n",
    "$$\n",
    "for scalar $x$ to obtain\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\sigma^2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\n",
    "&= \\mathbf{r}_i^T \\left(\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1}\\right)\\mathbf{r}_i\\\\\n",
    "&= -\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1}(\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i)\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\\\\n",
    "&= -\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient w.r.t $\\mathbf{L}$. Consider $\\boldsymbol{\\Sigma}$ first. By the general chain rule and the vectorization formula,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{vec}\\frac{\\partial}{\\partial\\boldsymbol{\\Sigma}}\n",
    "\\left(\\log \\det \\boldsymbol{\\Omega}_i + \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)\n",
    "&= \\left(D_{\\boldsymbol{\\Omega}_i}\\left(\\log \\det \\boldsymbol{\\Omega}_i + \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)D_{\\boldsymbol{\\Sigma}}\\boldsymbol{\\Omega}_i\\right)^T\\\\\n",
    "&= \n",
    "\\left\\{\\left[\\text{vec}\\frac{\\partial}{\\partial\\boldsymbol{\\Omega}}\\left(\\log \\det \\boldsymbol{\\Omega}_i\n",
    "+ \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)\\right]^T\n",
    "\\cdot D_{\\boldsymbol{\\Sigma}}\\mathbf{Z}_i\\boldsymbol{\\Sigma}\\mathbf{Z}_i^T\\right\\}^T\n",
    "\\\\\n",
    "&= \n",
    "\\left\\{\\left[\\text{vec}(\\boldsymbol{\\Omega}_i^{-1} - \\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1})\\right]^T\n",
    "\\cdot(\\mathbf{Z}_i\\otimes\\mathbf{Z}_i)\\right\\}^T\\\\\n",
    "&= (\\mathbf{Z}_i^T\\otimes \\mathbf{Z}_i^T)\n",
    "\\text{vec}(\\boldsymbol{\\Omega}_i^{-1} - \\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1})\\\\\n",
    "&= \\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i),\n",
    "\\end{aligned}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\Sigma}}\n",
    "\\left(\\log \\det \\boldsymbol{\\Omega}_i + \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)\n",
    "&= \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\\\\n",
    "\\Longrightarrow\\frac{\\partial}{\\partial\\boldsymbol{\\Sigma}}\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= -\\frac 1 2\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i + \\frac 1 2\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Finally show below:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\mathbf{L}}\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= 2\\frac{\\partial}{\\partial\\boldsymbol{\\Sigma}}\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\mathbf{L}\\\\\n",
    "&= - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "*Proof*:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{\\mathbf{L}}\\ell_i &= D_{\\boldsymbol{\\Sigma}}\\ell_iD_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= \\left(\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right)^T(\\mathbf{I}_{q^2} + \\mathbf{K}_{qq})(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= \\left(\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right)^T(\\mathbf{L}\\otimes\\mathbf{I}_q)\n",
    "+ \\left(\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right)^T\\mathbf{K}_{qq}(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= \\left[(\\mathbf{L}^T\\otimes\\mathbf{I}_q)\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right]^T\n",
    "+ \\left[(\\mathbf{L}^T\\otimes\\mathbf{I}_q)\\mathbf{K}_{qq}\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right]^T\n",
    "\\quad\\because \\mathbf{K}_{qq}^T = \\mathbf{K}_{qq}\\\\\n",
    "&= \\left[(\\mathbf{L}^T\\otimes\\mathbf{I}_q)\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right]^T\n",
    "+ \\left[(\\mathbf{L}^T\\otimes\\mathbf{I}_q)\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\right]^T\n",
    "\\quad\\because \\mathbf{K}_{qq}\\text{vec}(\\mathbf{A}) = \\text{vec}(\\mathbf{A}^T) \\ \\text{but} \\ \\boldsymbol{\\Sigma}^T = \\boldsymbol{\\Sigma}\\ \\\\\n",
    "&= 2\\left(\\text{vec}\\frac{\\partial\\ell_i}{\\partial\\boldsymbol{\\Sigma}}\\mathbf{L}\\right)^T\\\\\n",
    "&= 2D_{\\boldsymbol{\\Sigma}}(\\ell_i\\mathbf{L}).\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The Hessian is crazy! The following three blocks (Jacobian) are straightforward:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\nabla_{\\boldsymbol{\\beta}^T}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\n",
    "= -\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1}\\mathbf{X}_i \\in\\mathbb{R}^{p\\times p} \\\\\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\nabla_{\\sigma^2}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\n",
    "= -\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i\n",
    "= \\left(\\nabla^2_{\\sigma^2, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right)^T\\in\\mathbb{R}^{p\\times 1}\\\\\n",
    "\\nabla^2_{\\sigma^2, \\sigma^2}\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= - \\frac{1}{2} \\operatorname{tr} (\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T (\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-2})\\mathbf{r}_i\\\\\n",
    "&= \\frac{1}{2} \\operatorname{tr}(\\boldsymbol{\\Omega}_i^{-1}(\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i)\\boldsymbol{\\Omega}_i^{-1})\n",
    "+ \\frac{1}{2} \\mathbf{r}_i^T \\left[(\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1})\\boldsymbol{\\Omega}_i^{-1} + \\boldsymbol{\\Omega}_i^{-1}(\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1})\\right]\\mathbf{r}_i\\\\\n",
    "&= \\frac{1}{2} \\operatorname{tr}(\\boldsymbol{\\Omega}_i^{-2})\n",
    "-\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-3}\\mathbf{r}_i\\in\\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The remaining blocks are challenging to me. Note $\\mathbf{K}_{qq}(\\mathbf{L}\\otimes\\mathbf{I}_q) = (\\mathbf{I}_q\\otimes\\mathbf{L})\\mathbf{K}_{qq}$ (very important!).\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\frac{\\partial}{\\partial(\\text{vec}\\mathbf{L})^T}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\\\\n",
    "&= D_{\\mathbf{L}}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\\\\n",
    "&= D_{\\boldsymbol{\\Omega}_i}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\cdot\n",
    "D_{\\boldsymbol{\\Sigma}}\\boldsymbol{\\Omega}_i D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= (\\mathbf{r}_i^T\\otimes\\mathbf{X}_i^T)(-\\boldsymbol{\\Omega}_i^{-1}\\otimes\\boldsymbol{\\Omega}_i^{-1})(\\mathbf{Z}_i\\otimes\\mathbf{Z}_i)\n",
    "(\\mathbf{I}_{q^2} + \\mathbf{K}_{qq})(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= -(\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)(\\mathbf{I}_{q^2} + \\mathbf{K}_{qq})(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= -(\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)(\\mathbf{L}\\otimes\\mathbf{I}_q) - (\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\\mathbf{K}_{qq}(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= -(\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L}\\otimes\\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i) - (\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{X}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L})\\mathbf{K}_{qq}\n",
    "\\in\\mathbb{R}^{p\\times q^2},\\\\\n",
    "&= \\left(\\nabla^2_{\\text{vec}\\mathbf{L}, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right)^T,\\\\\n",
    "\\nabla^2_{\\sigma^2, \\text{vec}\\mathbf{L}}\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= - \\frac{1}{2}\\left[D_{\\mathbf{L}}\\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) - D_{\\mathbf{L}}\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i\\right]\\\\\n",
    "&= - \\frac{1}{2}\\left[D_{\\boldsymbol{\\Omega}_i}\\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) - D_{\\boldsymbol{\\Omega}_i}\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i\\right]\n",
    "D_{\\boldsymbol{\\Sigma}}\\boldsymbol{\\Omega}_i D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= - \\frac{1}{2}\\left[-(\\text{vec}(\\boldsymbol{\\Omega}_i^{-2}))^T - (\\text{vec}(2\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T))^T(-\\boldsymbol{\\Omega}_i^{-1}\\otimes \\boldsymbol{\\Omega}_i^{-1})\\right]\n",
    "(\\mathbf{Z}_i\\otimes\\mathbf{Z}_i) D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= - \\frac{1}{2}\\left[-(\\text{vec}(\\boldsymbol{\\Omega}_i^{-2}))^T(\\mathbf{Z}_i\\otimes\\mathbf{Z}_i) + (\\text{vec}(2\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T))^T(\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes \\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\\right]\n",
    " D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    " &= - \\frac{1}{2}\\left[-(\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i))^T + (\\text{vec}(2\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i))^T\\right]\n",
    " (\\mathbf{I}_{q^2} + \\mathbf{K}_{qq})(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    " &= - \\frac{1}{2}\\left[-2(\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L}))^T + 2(\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L} + \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L}))^T\\right]\\\\\n",
    " &= \\left(\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L})\\right)^T\n",
    " \\in\\mathbb{R}^{1\\times q^2}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "The above second equation can be easily verified by considering the transpose block:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla^2_{\\text{vec}\\mathbf{L}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\text{vec} \\left(- \\mathbf{Z}_i^T (\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1})\\mathbf{Z}_i \\mathbf{L}\n",
    "+ \\left[\\mathbf{Z}_i^T (\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1})\\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T (\\nabla_{\\sigma^2}\\boldsymbol{\\Omega}_i^{-1}) \\mathbf{Z}_i \\mathbf{L}\\right]\\right)\\\\\n",
    "&= \\text{vec}\\left(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i \\mathbf{L}\\right)\n",
    "- \\text{vec}\\left(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i \\mathbf{L}\\right)\\\\\n",
    "&= \\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L})\n",
    "\\in\\mathbb{R}^{q^2\\times 1}\\\\\n",
    "&= \\left(\\nabla^2_{\\sigma^2, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right)^T.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last block matrix should be the most complicated so let me keep $D_{\\mathbf{L}}\\boldsymbol{\\Sigma}$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla^2_{\\text{vec}\\mathbf{L}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\frac{\\partial\\text{vec}(- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})}{\\partial(\\text{vec}\\mathbf{L})^T}\\\\\n",
    "&= D_{\\mathbf{L}}(- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})\\\\\n",
    "&= D_{\\boldsymbol{\\Omega}_i}(- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})\n",
    "D_{\\boldsymbol{\\Sigma}}\\boldsymbol{\\Omega}_i D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= D_{\\boldsymbol{\\Omega}_i^{-1}}(- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})\n",
    "D_{\\boldsymbol{\\Omega}_i}\\boldsymbol{\\Omega}_i^{-1}\\cdot D_{\\boldsymbol{\\Sigma}}\\boldsymbol{\\Omega}_i D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= (- \\mathbf{L}^T\\mathbf{Z}_i^T \\otimes\\mathbf{Z}_i^T + \n",
    "(\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T\\otimes\\mathbf{Z}_i^T + \\mathbf{L}^T\\mathbf{Z}_i^T\\otimes \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T))\n",
    "(-\\boldsymbol{\\Omega}_i^{-1}\\otimes \\boldsymbol{\\Omega}_i^{-1})(\\mathbf{Z}_i\\otimes\\mathbf{Z}_i)D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i -\n",
    "\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i - \\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\n",
    "D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian for $i$-th datum can be expressed as\n",
    "$$\n",
    "\\mathbf{H}(\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)) = \n",
    "\\begin{bmatrix}\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\boldsymbol{\\beta}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\\\\n",
    "\\nabla^2_{\\sigma^2, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\sigma^2, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\sigma^2, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\\\\n",
    "\\nabla^2_{\\text{vec}\\mathbf{L}, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\text{vec}\\mathbf{L}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &\n",
    "\\nabla^2_{\\text{vec}\\mathbf{L}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "\\end{bmatrix}\n",
    "\\in\\mathbb{R}^{(p+1+q^2)\\times(p+1+q^2)}\n",
    "$$\n",
    "and the observed information matrix and the expected (Fisher) information matrix are\n",
    "$$\n",
    "-\\mathbf{H}(\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)), \\quad\n",
    "E\\left[-\\mathbf{H}(\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2))\\right],\n",
    "$$\n",
    "respectively. Taking expectation makes some terms canceled out and simplified due to $E(\\mathbf{r}_i) = \\mathbf{0}$ and $E(\\mathbf{r}_i\\mathbf{r}_i^T) = \\boldsymbol{\\Omega}_i$. Specifically,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[-\\nabla^2_{\\boldsymbol{\\beta}, \\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= E(\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1}\\mathbf{X}_i) = \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1}\\mathbf{X}_i\\\\\n",
    "E\\left[-\\nabla^2_{\\boldsymbol{\\beta}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= E(\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i) = \\mathbf{0},\\\\\n",
    "E\\left[-\\nabla^2_{\\boldsymbol{\\beta}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= E\\left[-D_{\\mathbf{L}}\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right]\n",
    "= \\mathbf{O}\\\\\n",
    "E\\left[-\\nabla^2_{\\sigma^2, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= E\\left[\\frac{1}{2} \\operatorname{tr}(\\boldsymbol{\\Omega}_i^{-2})\n",
    "-\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-3}\\mathbf{r}_i\\right] = -\\frac{1}{2} \\operatorname{tr}(\\boldsymbol{\\Omega}_i^{-2})\\\\\n",
    "&\\because E(\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-3}\\mathbf{r}_i) = E\\left[\\text{tr}(\\boldsymbol{\\Omega}_i^{-3}\\mathbf{r}_i\\mathbf{r}_i^T)\\right] = \\text{tr}(\\boldsymbol{\\Omega}_i^{-3}E(\\mathbf{r}_i\\mathbf{r}_i^T)) = \\text{tr}(\\boldsymbol{\\Omega}_i^{-2})\\\\\n",
    "E\\left[-\\nabla^2_{\\text{vec}\\mathbf{L}, \\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= -\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}E(\\mathbf{r}_i\\mathbf{r}_i^T)\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}E(\\mathbf{r}_i\\mathbf{r}_i^T)\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L})\\\\\n",
    "&= -\\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L} - \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L})\\\\\n",
    "&= \\text{vec}(\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{Z}_i\\mathbf{L})\\\\\n",
    "E\\left[-\\nabla^2_{\\text{vec}\\mathbf{L}, \\text{vec}\\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\\right]\n",
    "&= -(\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i -\n",
    "\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i - \\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\otimes \\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\n",
    "D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\n",
    "D_{\\mathbf{L}}\\boldsymbol{\\Sigma}\\\\\n",
    "&= (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\n",
    "(\\mathbf{I}_{q^2} + \\mathbf{K}_{qq})(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)(\\mathbf{L}\\otimes\\mathbf{I}_q)\n",
    "+ (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\\mathbf{K}_{qq}(\\mathbf{L}\\otimes\\mathbf{I}_q)\\\\\n",
    "&= (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L} \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i)\n",
    "+ (\\mathbf{L}^T\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i \\otimes\\mathbf{Z}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{Z}_i\\mathbf{L})\\mathbf{K}_{qq}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "I learned a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum\n",
    "\n",
    "We expand the code from HW3 to evaluate both objective and gradient. I provide my code for HW3 below as a starting point. You do _not_ have to use this code. If your come up faster code, that's even better. \n",
    "\n",
    "**Solution**\n",
    "\n",
    "We first rewrite the three gradient expressions for efficient computation using\n",
    "$\n",
    "\\mathbf{\\Omega}_i^{-1}\n",
    "= \\sigma^{-2} \\mathbf I_{n_i} - \\sigma^{-4}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T,\n",
    "$\n",
    "where $\\mathbf{M}\\mathbf{M}^T = \\mathbf{I}_q + \\sigma^{-2}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\\\\n",
    "&= \\sigma^{-2}\\mathbf{X}_i^T\\mathbf{r}_i -\n",
    "\\sigma^{-4}\\mathbf{X}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\\\\n",
    "&= \\sigma^{-2}\\mathbf{X}_i^T\\mathbf{r}_i\n",
    "- \\sigma^{-4}\\mathbf{X}_i^T\\mathbf{Z}_i\\mathbf{L}\\mathbf{M}^{-T}\\mathbf{M}^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\mathbf{X}_i^T\\mathbf{r}_i$ and $\\mathbf{Z}_i^T\\mathbf{r}_i$ are stored in `storage_p` and `storage_q`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)$, consider each term separately and merge them:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1})\n",
    "&= \\operatorname{tr} \\left(\\sigma^{-2} \\mathbf I_{n_i} - \\sigma^{-4}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right)\\\\\n",
    "&= n_i\\sigma^{-2} - \\sigma^{-4}\\operatorname{tr} \\left(\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right)\\\\\n",
    "&= n_i\\sigma^{-2} - \\sigma^{-4}\\operatorname{tr} \\left(\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\right),\\\\\n",
    "&= n_i\\sigma^{-2} - \\sigma^{-4}\\operatorname{dot} \\left(\\mathbf{Z}_i^T\\mathbf{Z}_i, \\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\right),\\\\\n",
    "\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i\n",
    "&= \\|\\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\|_2^2\\\\\n",
    "&= \\left\\|\\left(\\sigma^{-2} \\mathbf I_{n_i} - \\sigma^{-4}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right)\\mathbf{r}_i\\right\\|_2^2\\\\\n",
    "&= \\sigma^{-4}\\left\\|\\left(\\mathbf I_{n_i} - \\sigma^{-2}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right)\\mathbf{r}_i\\right\\|_2^2\\\\\n",
    "&= \\sigma^{-4}\\left[\\mathbf{r}_i^T\\mathbf{r}_i -2\\sigma^{-2}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\n",
    "+ \\sigma^{-4}\\mathbf{r}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\right]\\\\\n",
    "&= \\sigma^{-4}\\left[\\mathbf{r}_i^T\\mathbf{r}_i - \\sigma^{-2}\\left(\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\right)^T\n",
    "\\left(2\\mathbf{Z}_i^T\\mathbf{r}_i - \\sigma^{-2}\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i\\\\\n",
    "&= - \\frac{n_i\\sigma^{-2}}{2} + \\frac{\\sigma^{-4}}{2}\\operatorname{dot} \\left(\\mathbf{Z}_i^T\\mathbf{Z}_i, \\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\right)\\\\\n",
    "&\\quad + \\frac{\\sigma^{-4}}{2}\\left[\\mathbf{r}_i^T\\mathbf{r}_i - \\sigma^{-2}\\left(\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\right)^T\n",
    "\\left(2\\mathbf{Z}_i^T\\mathbf{r}_i - \\sigma^{-2}\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\mathbf{r}_i\\right)\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\mathbf{r}_i^T\\mathbf{r}_i = \\mathbf{y}_i^T\\mathbf{y}_i + \\boldsymbol{\\beta}^T(\\mathbf{X}_i^T\\mathbf{X}_i\\boldsymbol{\\beta} - 2\\mathbf{X}_i^T\\mathbf{y}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)\n",
    "&= - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i\\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i\\mathbf{L}\\\\\n",
    "&= - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i\\mathbf{L} + \\left(\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right) \\left(\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)^T\\mathbf{L}\\\\\n",
    "&= - \\mathbf{Z}_i^T\\left[\\sigma^{-2} \\mathbf I_{n_i} - \\sigma^{-4}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right]\\mathbf{Z}_i\\mathbf{L}\n",
    "+ \\left(\\mathbf{Z}_i^T \\left[\\sigma^{-2} \\mathbf I_{n_i} - \\sigma^{-4}\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\mathbf{Z}_i^T\\right] \\mathbf{r}_i\\right) \\left(\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)^T\\mathbf{L}\\\\\n",
    "&= - \\left[\\sigma^{-2} \\mathbf I_{q} - \\sigma^{-4}\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\right] \\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}\n",
    "+ \\left[\\sigma^{-2} \\mathbf I_{q} - \\sigma^{-4}\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T\\right] \\mathbf{Z}_i^T\\mathbf{r}_i \\left(\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\\right)^T\\mathbf{L}\\\\\n",
    "&= -\\mathbf{W}_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L} + \\mathbf{W}_i\\mathbf{Z}_i^T\\mathbf{r}_i(\\mathbf{W}_i\\mathbf{Z}_i^T\\mathbf{r}_i)^T\\mathbf{L},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\mathbf{W}_i = \\sigma^{-2} \\mathbf I_{q} - \\sigma^{-4}\\mathbf{Z}_i^T\\mathbf{Z}_i\\mathbf{L}(\\mathbf{M}\\mathbf{M}^T)^{-1}\\mathbf{L}^T$, which is *not* symmetric.\n",
    "\n",
    "Use my code for `log!()` developed in HW3. I further pre-allocate `storage_q2` and `storage_qq2` for the gradients. Note that a few lines exceed 80 characters, but remember that the criterion for the course is 92 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇Σ         :: Matrix{T}  \n",
    "    ∇L         :: Matrix{T} # added\n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2 :: Vector{T} # added\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T} # added\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "    y          ::Vector{T}, \n",
    "    X          ::Matrix{T}, \n",
    "    Z          ::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇Σ         = Matrix{T}(undef, q, q)\n",
    "    ∇L         = Matrix{T}(undef, q, q) # added\n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2 = Vector{T}(undef, q) # added\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz) # added\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, ∇L,\n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2, \n",
    "        xtx, ztx, ztz, storage_qq, storage_qq2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad==true`, then `obs.∇β`, `obs.∇Σ`, and `obs.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "    obs      :: LmmObs{T}, \n",
    "    β        :: Vector{T}, \n",
    "    L        :: Matrix{T}, \n",
    "    σ²       :: T,\n",
    "    needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    σ²inv   = inv(σ²)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    ## 1st term = -(n/2) log(2πσ²) ----------------------------------------- ##\n",
    "    LogLik = -(n//2) * log(2π * σ²)\n",
    "\n",
    "    ## 2nd term = -logdet(M), where M = L'Z'ZL / σ² + I -------------------- ##\n",
    "    mul!(obs.storage_qq, transpose(L), obs.ztz) # L'Z'Z\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', σ²inv, L, obs.storage_qq) # (L'Z'Z)L / σ²\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += 1.0 # L'Z'ZL / σ² + I\n",
    "    end\n",
    "    LAPACK.potrf!('L', obs.storage_qq) # M = cholesky!(Symmetric(obs.storage_qq, :L))\n",
    "    LogLik -= logdet(LowerTriangular(obs.storage_qq)) # logdet(M)\n",
    "\n",
    "    ## 3rd term = -1/2 (||y||^2 + β'(X'Xβ - 2X'y)) / σ² -------------------- ##\n",
    "    obs.storage_p .= obs.xty # X'y\n",
    "    BLAS.symv!('U', T(1), obs.xtx, β, T(-2), obs.storage_p) # X'Xβ - 2X'y\n",
    "    LogLik -= (1//2) * (obs.yty + dot(β, obs.storage_p)) * σ²inv\n",
    "\n",
    "    ## 4th term = 1/2 ||M⁻¹(L'Z'y - L'Z'Xβ)||² / (σ²)² --------------------- ##\n",
    "    obs.storage_q .= obs.zty                                # Z'y\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), obs.storage_q) # Z'y - Z'Xβ = Z'r\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q)             # L'Z'r\n",
    "    BLAS.trsv!('L', 'N', 'N', obs.storage_qq, obs.storage_q) # M⁻¹L'Z'r\n",
    "    LogLik += (1//2) * abs2(norm(obs.storage_q)) * σ²inv^2\n",
    "    \n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################   \n",
    "    if needgrad\n",
    "        # TODO: fill ∇β, ∇Σ, ∇L, ∇σ² by gradients\n",
    "        # ∇β ---------------------------------------------------------------- # \n",
    "        BLAS.symv!('U', T(-1), obs.xtx, β, T(1), copy!(obs.∇β, obs.xty)) # X'y - X'Xβ = X'r\n",
    "        BLAS.trsv!('L', 'T', 'N', obs.storage_qq, obs.storage_q) # M⁻ᵀ(M⁻¹L'Z'r)\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q)              # L(M⁻ᵀM⁻¹L'Z'r)\n",
    "        BLAS.gemv!('T', -σ²inv^2, obs.ztx, obs.storage_q, σ²inv, obs.∇β) # ∇β\n",
    "\n",
    "        # 2∇Σ (not ∇Σ) ------------------------------------------------------ #\n",
    "        ## 1st term = -WZ'Z ------------------------------------------------ ##\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, copy!(obs.storage_qq2, obs.ztz)) # Z'ZL\n",
    "        BLAS.trsm!('R', 'L', 'T', 'N', T(1), obs.storage_qq, obs.storage_qq2) # Z'ZLM⁻ᵀ\n",
    "        BLAS.trsm!('R', 'L', 'N', 'N', T(1), obs.storage_qq, obs.storage_qq2) # Z'ZLM⁻ᵀM⁻¹\n",
    "        BLAS.trmm!('R', 'L', 'T', 'N', T(-σ²inv), L, obs.storage_qq2)         # -σ⁻²Z'ZLM⁻ᵀM⁻¹L'\n",
    "        @inbounds for j in 1:q\n",
    "            obs.storage_qq2[j, j] += 1.0 # I - σ⁻²Z'ZLM⁻ᵀM⁻¹L\n",
    "        end\n",
    "        obs.storage_qq2 .*= σ²inv # σ⁻²I - σ⁻⁴Z'ZLM⁻ᵀM⁻¹L' = W (asymmetric)\n",
    "        BLAS.symm!('R', 'U', T(-1), obs.ztz, obs.storage_qq2 , T(0), obs.∇Σ) # -WZ'Z\n",
    "        ## 2nd term = (WZtr)(WZtr)' ---------------------------------------- ##\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.storage_q, obs.zty)) # Z'r\n",
    "        BLAS.gemv!('N', T(1), obs.storage_qq2, obs.storage_q, T(0), obs.storage_q2) # WZ'r\n",
    "        ### sum of the two terms after outer product\n",
    "        BLAS.ger!(T(1), obs.storage_q2, obs.storage_q2, obs.∇Σ) # 2∇Σ\n",
    "        # ∇L ---------------------------------------------------------------- #\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, copy!(obs.∇L, obs.∇Σ)) # 2∇ΣL\n",
    "\n",
    "        # ∇σ² --------------------------------------------------------------- #\n",
    "        ## 1st term (constant) --------------------------------------------- ##\n",
    "        obs.∇σ² .= -(n//2) * σ²inv\n",
    "        ## 2nd term (dot product) ------------------------------------------ ##\n",
    "        BLAS.trsm!('R', 'L', 'T', 'N', T(1), obs.storage_qq, copy!(obs.storage_qq2, L)) # LM⁻ᵀ\n",
    "        BLAS.trsm!('R', 'L', 'N', 'N', T(1), obs.storage_qq, obs.storage_qq2) # LM⁻ᵀM⁻¹\n",
    "        BLAS.trmm!('R', 'L', 'T', 'N', T(1), L, obs.storage_qq2) # LM⁻ᵀM⁻¹L'\n",
    "        obs.∇σ² .+= (1//2) * σ²inv^2 * dot(obs.ztz, obs.storage_qq2) # σ⁻⁴tr(Z'ZLM⁻ᵀM⁻¹L')\n",
    "        ## 3rd term (norm) ------------------------------------------------- ##\n",
    "        BLAS.symv!('L', T(1), obs.storage_qq2, obs.storage_q, T(0), obs.storage_q2) # LM⁻ᵀM⁻¹L'Z'r\n",
    "        BLAS.symv!('L', -σ²inv, obs.ztz, obs.storage_q2, T(2), obs.storage_q)\n",
    "        obs.∇σ² .+= (1//2) * σ²inv^2 * (obs.yty + dot(β, obs.storage_p) # r'r\n",
    "                    - σ²inv * dot(obs.storage_q2, obs.storage_q)) # remaining term\n",
    "    end  \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return LogLik\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in [HW3](https://ucla-biostat-257.github.io/2024spring/hw/hw3/hw03.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3256.1793358058385\n",
      "obs.∇β = [0.2669810805700901, 41.61418337067374, -34.346649623127014, 36.108985107075085, 27.913948208793503]\n",
      "obs.∇σ² = [1.6283715138490606]\n",
      "obs.∇Σ = [-0.9464482950697549 0.057792444809472696 -0.3024412763919055; 0.05779244480946695 -1.0008716491712542 0.2845116557145112; -0.30244127639187135 0.28451165571452075 1.1700409272598846]\n",
      "obs.∇L = [-0.9709131782279983 0.030145913774881482 -0.2996791977474491; -0.013843554536207352 -0.9701196695228238 0.2819133213280048; -0.15698601809443083 0.38891970710309437 1.1593553981651827]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇Σ\n",
    "@show obs.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`.\n",
    "\n",
    "**Comment**\n",
    "\n",
    "It is important to note that the values for `obs.∇Σ` by Hua are twice as the actual values because he (and I accordingly) did not multiply it by 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3256.1793358058258)) < 1e-4\n",
    "@assert norm(obs.∇β - [0.26698108057144054, 41.61418337067327, \n",
    "        -34.34664962312689, 36.10898510707527, 27.913948208793144]) < 1e-4\n",
    "@assert norm(obs.∇Σ - \n",
    "    [-0.9464482950697888 0.057792444809492895 -0.30244127639188767; \n",
    "        0.057792444809492895 -1.00087164917123 0.2845116557144694; \n",
    "        -0.30244127639188767 0.2845116557144694 1.170040927259726]) < 1e-4\n",
    "@assert abs(obs.∇σ²[1] - (1.6283715138412163)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Benchmark for evaluating objective only. This is what we did in HW3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 140 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m710.421 ns\u001b[22m\u001b[39m … \u001b[35m1.074 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m713.686 ns             \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m714.516 ns\u001b[22m\u001b[39m ± \u001b[32m6.069 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[34m▇\u001b[39m\u001b[39m▆\u001b[32m▆\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  710 ns\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       732 ns \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for objective + gradient evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 9 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m2.574 μs\u001b[22m\u001b[39m … \u001b[35m 4.218 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m2.625 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m2.626 μs\u001b[22m\u001b[39m ± \u001b[32m33.391 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▄\u001b[39m \u001b[39m \u001b[39m▅\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \u001b[39m▆\u001b[39m \u001b[39m \u001b[39m▅\u001b[39m \u001b[39m \u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m \u001b[39m▃\n",
       "  2.57 μs\u001b[90m        Histogram: frequency by time\u001b[39m        2.67 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 900ns. You will get full credit (10 pts) if the median run time is within 10μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a `LmmModel` object is simply the sum of log-likelihood/gradient of individual data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MOI.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    storage_2q :: Vector{T} # added to store ztr2 for each i\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "    storage_2q2q :: Matrix{T} # added to store ztz2 for each i\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    storage_2q = similar(ztr2) # added\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    storage_2q2q = similar(ztz2) # added\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, storage_2q, xtx, ztz2, storage_2q2q)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad==true`, then `m.∇β`, `m.∇Σ`, and `m.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            # BLAS.axpy!(T(1), obs.∇Σ, m.∇L) # I defined ∇L\n",
    "            BLAS.axpy!(T(1), obs.∇L, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "\n",
    "Let's generate a synthetic longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5; zeros(p - 5)]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0; zeros(q - 3)]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue), Val(true), check=false).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file `lmm_data.csv`. **Do not put this file in Git.** It takes 245.4MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(isfile(\"lmm_data.csv\") && filesize(\"lmm_data.csv\") == 245369685) || \n",
    "open(\"lmm_data.csv\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Evaluate log-likelihood and gradient of whole data set at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.8400684383699736e6\n",
      "lmm.∇β = [41.06591670742383, 445.7512035395316, 157.0133992248792, -335.0997736073277, -895.6257448387724]\n",
      "lmm.∇σ² = [-489.53617303727367]\n",
      "lmm.∇L = [-3.398257593515736 31.32103842087272 26.736450897355372; 40.4352867299875 61.86377650461247 -75.37427770755431; 37.81105146876211 -82.5683843121727 -56.459925427587095]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 20 points if following code throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert abs(obj - (-2.840068438369969e6)) < 1e-4\n",
    "@assert norm(lmm.∇β - [41.0659167074073, 445.75120353972426, \n",
    "        157.0133992249258, -335.09977360733626, -895.6257448385899]) < 1e-4\n",
    "@assert norm(lmm.∇L - [-3.3982575935824837 31.32103842086001 26.73645089732865; \n",
    "        40.43528672997116 61.86377650461202 -75.37427770754684; \n",
    "        37.811051468724486 -82.56838431216435 -56.45992542754974]) < 1e-4\n",
    "@assert abs(lmm.∇σ²[1] - (-489.5361730382465)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Test efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 1882 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m2.631 ms\u001b[22m\u001b[39m … \u001b[35m 4.689 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m2.641 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m2.655 ms\u001b[22m\u001b[39m ± \u001b[32m74.391 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▆\u001b[39m█\u001b[34m▅\u001b[39m\u001b[39m▃\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▆\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▅\u001b[39m▆\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m \u001b[39m█\n",
       "  2.63 ms\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m     2.99 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 1.4ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. (30 pts) Starting point\n",
    "\n",
    "For numerical optimization, a good starting point is critical. Let's start $\\boldsymbol{\\beta}$ and $\\sigma^2$ from the least squares solutions (ignoring intra-individual correlations)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\beta}^{(0)} &= \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)^{-1} \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{y}_i\\right) \\\\\n",
    "\\sigma^{2(0)} &= \\frac{\\sum_i \\|\\mathbf{r}_i^{(0)}\\|_2^2}{\\sum_i n_i} = \\frac{\\sum_i \\|\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}^{(0)}\\|_2^2}{\\sum_i n_i}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "To get a reasonable starting point for $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$, we can minimize the least squares criterion (ignoring the noise variance component)\n",
    "$$\n",
    "    \\text{minimize} \\sum_i \\|\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2.\n",
    "$$\n",
    "Derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$ (10 pts). \n",
    "\n",
    "We implement this start point strategy in the function `init_ls()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "We can obtain $\\boldsymbol{\\beta}^{(0)}$ by solving the following linear system in the form of $\\mathbf{Ax}=\\mathbf{b}$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)\\boldsymbol{\\beta}^{(0)} = \\sum_i \\mathbf{X}_i^T \\mathbf{y}_i.\n",
    "\\end{aligned}\n",
    "$$\n",
    "For $\\sigma^{2(0)}$, we calculate\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma^{2(0)} = \\frac{\\sum_i\\left[\\mathbf{y}_i^T\\mathbf{y}_i + \\boldsymbol{\\beta}^{(0)T}(\\mathbf{X}_i^T\\mathbf{X}_i\\boldsymbol{\\beta}^{(0)} - 2\\mathbf{X}_i^T\\mathbf{y}_i)\\right]}{\\sum_i n_i}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive $\\boldsymbol{\\Sigma}^{(0)}$, we need the gradient and hessian. Using the following formula \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial\\|\\mathbf{C} - \\mathbf{AXB}\\|_\\text{F}^2}{\\partial\\mathbf{X}}\n",
    "&= \\frac{\\partial}{\\partial\\mathbf{X}}\\text{tr}\\left((\\mathbf{C} - \\mathbf{A\\Sigma B})(\\mathbf{C} - \\mathbf{AXB})^T\\right)\\\\\n",
    "&= \\frac{\\partial}{\\partial\\mathbf{X}}\\text{tr}(\\mathbf{C}\\mathbf{C}^T - \\mathbf{C}\\mathbf{B}^T\\mathbf{X}^T\\mathbf{A}^T - \\mathbf{AXB}\\mathbf{C}^T + \\mathbf{AXB}\\mathbf{B}^T\\mathbf{X}^T\\mathbf{A}^T)\\\\\n",
    "&= \\mathbf{O} - \\mathbf{A}^T\\mathbf{C}\\mathbf{B}^T - \\mathbf{A}^T\\mathbf{C}\\mathbf{B}^T + (\\mathbf{A}^T\\mathbf{AXB}\\mathbf{B}^T + \\mathbf{A}^T\\mathbf{AXB}\\mathbf{B}^T)\\\\\n",
    "&= -2\\mathbf{A}^T(\\mathbf{C} - \\mathbf{AXB}) \\mathbf{B}^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "yields our gradient\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial\\sum_i\\|\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2}{\\partial\\mathbf{\\Sigma}}\n",
    "= -2\\sum_i\\left[\\mathbf{Z}_i^T(\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T ) \\mathbf{Z}_i\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "and then the hessian\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2\\sum_i\\|\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2}{\\partial\\mathbf{\\Sigma}^2}\n",
    "= 2\\frac{\\partial\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{Z}_i}{\\partial\\mathbf{\\Sigma}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "should be psd. Therefore, taking the derivative w.r.t $\\mathbf{\\Sigma}$ equal to zero gives the minimizer, such that\n",
    "$$\n",
    "\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\boldsymbol{\\Sigma}^{(0)}\\mathbf{Z}_i^T\\mathbf{Z}_i = \\sum_i(\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)})(\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)})^T.\n",
    "$$\n",
    "To solve this, we need its (column-major) vectorization. Since $\\text{vec}(\\mathbf{ABC}) = (\\mathbf{C}^T\\otimes\\mathbf{A})\\text{vec}(\\mathbf{B})$, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{vec}\\left(\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\boldsymbol{\\Sigma}^{(0)}\\mathbf{Z}_i^T\\mathbf{Z}_i\\right)\n",
    "&= \\text{vec}\\left(\\sum_i(\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)})(\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)})^T\\right)\\\\\n",
    "\\Longrightarrow\\underbrace{\\left(\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\otimes\\mathbf{Z}_i^T\\mathbf{Z}_i\\right)}_{q^2\\times q^2}\n",
    "\\underbrace{\\text{vec}(\\boldsymbol{\\Sigma}^{(0)})}_{q^2\\times 1}\n",
    "&= \\underbrace{\\left(\\sum_i \\mathbf{Z}_i^T\\mathbf{r}_i^{(0)}\\otimes\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)}\\right)}_{q^2\\times 1}\\\\\n",
    "\\Longrightarrow\\text{vec}(\\boldsymbol{\\Sigma}^{(0)})\n",
    "&= \\left(\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\otimes\\mathbf{Z}_i^T\\mathbf{Z}_i\\right)^{-1}\\left(\\sum_i \\mathbf{Z}_i^T\\mathbf{r}_i^{(0)}\\otimes\\mathbf{Z}_i^T\\mathbf{r}_i^{(0)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "with $\\boldsymbol{\\Sigma}^{(0)} = \\mathbf{L}^{(0)}\\mathbf{L}^{(0)T}$ by the Cholesky decomposition.\n",
    "\n",
    "In coding, we can estimate $\\sigma^{2(0)}$ and $\\boldsymbol{\\Sigma}^{(0)}$ simultaneously once $\\boldsymbol{\\beta}^{(0)}$ is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    # TODO: fill m.β, m.L, m.σ² by LS estimates\n",
    "    # m.β ------------------------------------------------------------------- #\n",
    "    fill!(m.xtx, 0)\n",
    "    fill!(m.xty, 0)\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        m.xtx .+= obs.xtx # ΣX'X\n",
    "        m.xty .+= obs.xty # ΣX'y\n",
    "    end\n",
    "    LAPACK.posv!('L', m.xtx, m.xty) # solve (ΣX'X)β = ΣX'y\n",
    "    copy!(m.β, m.xty) # copy solution to m.β\n",
    "\n",
    "    # m.σ² and m.L ---------------------------------------------------------- #\n",
    "    fill!(m.σ², 0)\n",
    "    fill!(m.ztz2, 0)\n",
    "    fill!(m.ztr2, 0)\n",
    "    # fill!(m.xty, 0) # cannot reuse m.xty since m.β is overwritten\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        ## m.σ² ------------------------------------------------------------ ##\n",
    "        BLAS.symv!('U', T(1), obs.xtx, m.β, T(-2), copy!(obs.storage_p, obs.xty)) # X'Xβ⁽⁰⁾ - 2X'y\n",
    "        m.σ² .+= obs.yty .+ dot(m.β, obs.storage_p) # sum(r'r)\n",
    "        ## m.L ------------------------------------------------------------- ##\n",
    "        BLAS.kron!(m.storage_2q2q, obs.ztz, obs.ztz) # Z'Z⊗Z'Z\n",
    "        m.ztz2 .+= m.storage_2q2q # sum(Z'Z⊗Z'Z)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, m.β, T(1), copy!(obs.storage_q, obs.zty)) # Z'r\n",
    "        BLAS,kron!(m.storage_2q, obs.storage_q, obs.storage_q) # Z'r⊗Z'r\n",
    "        m.ztr2 .+= m.storage_2q # sum(Z'r⊗Z'r)\n",
    "    end\n",
    "    m.σ² ./= sum(length(obs.y) for obs in m.data) # sum(r'r) / sum(n)\n",
    "    LAPACK.posv!('L', m.ztz2, m.ztr2) # solve sum(Z'Z⊗Z'Z)vec(Σ) = sum(Z'r⊗Z'r)\n",
    "    # m.L .= reshape(m.ztr2, q, q) # this takes 2 allocations\n",
    "    @inbounds for i in 1:q, j in 1:q\n",
    "        m.L[i, j] = m.ztr2[(j-1) * q + i] # copy solution to L (but still Σ)\n",
    "    end\n",
    "    LAPACK.potrf!('L', m.L) # Cholesky factor L\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.356237077697068e6\n",
      "lmm.β = [0.18207934611476326, 6.50048070099372, -3.4979107842091586, 1.001113296229795, 5.0002519857919285]\n",
      "lmm.σ² = [5.709004733413663]\n",
      "lmm.L = [1.4069222734993236 0.07258461003916689 0.05717147035274019; 0.05159105901325529 1.131979211870369 -0.07707942768978568; 0.0406358413891211 -0.06994463586493146 0.9718256360134827]\n"
     ]
    }
   ],
   "source": [
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Your start points should have a log-likelihood larger than -3.3627e6 (10 pts). The points you get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.3627e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "The start point should be computed quickly. Otherwise there is no point using it as a starting point. My median run time is 175μs. You get full credit (10 pts) if the median run time is within 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 9721 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m509.125 μs\u001b[22m\u001b[39m … \u001b[35m820.917 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m512.166 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m512.894 μs\u001b[22m\u001b[39m ± \u001b[32m  6.156 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m▃\u001b[39m▇\u001b[39m█\u001b[39m▅\u001b[39m▃\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▆\u001b[32m▄\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m \u001b[39m█\n",
       "  509 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        536 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. NLP via MathOptInterface.jl\n",
    "\n",
    "We define the NLP problem using the modelling tool [MathOptInterface.jl](https://github.com/jump-dev/MathOptInterface.jl). Start-up code is given below. Modify if necessary to accomodate your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver=Ipopt.Optimizer())\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m :: LmmModel{T},\n",
    "        solver = Ipopt.Optimizer()\n",
    "    ) where T <: AbstractFloat\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    # prep the MOI\n",
    "    MOI.empty!(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # q diagonal entries of Cholesky factor L should be >= 0\n",
    "    # σ² should be >= 0\n",
    "    lb   = fill(0.0, q + 1)\n",
    "    ub   = fill(Inf, q + 1)\n",
    "    NLPBlock = MOI.NLPBlockData(MOI.NLPBoundsPair.(lb, ub), m, true)\n",
    "    MOI.set(solver, MOI.NLPBlock(), NLPBlock)\n",
    "    # start point\n",
    "    params = MOI.add_variables(solver, npar)    \n",
    "    par0   = Vector{T}(undef, npar)\n",
    "    modelpar_to_optimpar!(par0, m)    \n",
    "    for i in 1:npar\n",
    "        MOI.set(solver, MOI.VariablePrimalStart(), params[i], par0[i])\n",
    "    end\n",
    "    MOI.set(solver, MOI.ObjectiveSense(), MOI.MAX_SENSE)\n",
    "    # optimize\n",
    "    MOI.optimize!(solver)\n",
    "    optstat = MOI.get(solver, MOI.TerminationStatus())\n",
    "    optstat in (MOI.LOCALLY_SOLVED, MOI.ALMOST_LOCALLY_SOLVED) || \n",
    "        @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    xsol = [MOI.get(solver, MOI.VariablePrimal(), MOI.VariableIndex(i)) for i in 1:npar]\n",
    "    optimpar_to_modelpar!(m, xsol)\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    ◺(n::Integer)\n",
    "\n",
    "Triangular number `n * (n + 1) / 2`.\n",
    "\"\"\"\n",
    "@inline ◺(n::Integer) = (n * (n + 1)) >> 1\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(par, m.β)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ²\n",
    "    par[end] = m.σ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(m.β, 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # σ²\n",
    "    m.σ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MOI.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in MOI.features_available(m))\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MOI.features_available(m::LmmModel) = [:Grad, :Hess, :Jac]\n",
    "\n",
    "function MOI.eval_objective(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MOI.eval_objective_gradient(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt β\n",
    "    copyto!(grad, m.∇β)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.∇L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to σ²\n",
    "    grad[end] = m.∇σ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "function MOI.eval_constraint(m::LmmModel, g, par)\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    gidx   = 1\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        if i == j\n",
    "            g[gidx] = par[offset]\n",
    "            gidx   += 1\n",
    "        end\n",
    "        offset += 1\n",
    "    end\n",
    "    g[end] = par[end]\n",
    "    g\n",
    "end\n",
    "\n",
    "function MOI.jacobian_structure(m::LmmModel)\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    row  = collect(1:(q + 1))\n",
    "    col  = Int[]\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        (i == j) && push!(col, offset)\n",
    "        offset += 1\n",
    "    end\n",
    "    push!(col, offset)\n",
    "    [(row[i], col[i]) for i in 1:length(row)]\n",
    "end\n",
    "\n",
    "MOI.eval_constraint_jacobian(m::LmmModel, J, par) = fill!(J, 1)\n",
    "\n",
    "function MOI.hessian_lagrangian_structure(m::LmmModel)\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)    \n",
    "    q◺   = ◺(q)\n",
    "    # we work on the upper triangular part of the Hessian\n",
    "    arr1 = Vector{Int}(undef, ◺(p) + ◺(q◺) + q◺ + 1)\n",
    "    arr2 = Vector{Int}(undef, ◺(p) + ◺(q◺) + q◺ + 1)\n",
    "    # Hββ block\n",
    "    idx  = 1    \n",
    "    for j in 1:p, i in 1:j\n",
    "        arr1[idx] = i\n",
    "        arr2[idx] = j\n",
    "        idx      += 1\n",
    "    end\n",
    "    # HLL block\n",
    "    for j in 1:q◺, i in 1:j\n",
    "        arr1[idx] = p + i\n",
    "        arr2[idx] = p + j\n",
    "        idx      += 1\n",
    "    end\n",
    "    # HLσ² block\n",
    "    for i in (p + 1):(p + q◺)\n",
    "        arr1[idx] = i\n",
    "        arr2[idx] = p + q◺ + 1\n",
    "        idx      += 1\n",
    "    end\n",
    "    # Hσ²σ² block\n",
    "    arr1[idx] = p + q◺ + 1\n",
    "    arr2[idx] = p + q◺ + 1\n",
    "    [(arr1[i], arr2[i]) for i in 1:length(arr1)]\n",
    "end\n",
    "\n",
    "function MOI.eval_hessian_lagrangian(\n",
    "        m   :: LmmModel, \n",
    "        H   :: AbstractVector{T},\n",
    "        par :: AbstractVector{T}, \n",
    "        σ   :: T, \n",
    "        μ   :: AbstractVector{T}\n",
    "    ) where {T}    \n",
    "    p  = size(m.data[1].X, 2)\n",
    "    q  = size(m.data[1].Z, 2)    \n",
    "    q◺ = ◺(q)\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, true, true)\n",
    "    # Hββ block\n",
    "    idx = 1    \n",
    "    @inbounds for j in 1:p, i in 1:j\n",
    "        H[idx] = m.Hββ[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # HLL block\n",
    "    @inbounds for j in 1:q◺, i in 1:j\n",
    "        H[idx] = m.HLL[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # HLσ² block\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        H[idx] = m.Hσ²L[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # Hσ²σ² block\n",
    "    H[idx] = m.Hσ²σ²[1, 1]\n",
    "    lmul!(σ, H)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. (20 pts) Test drive\n",
    "\n",
    "Now we can run any NLP solver (supported by MathOptInterface.jl) to compute the MLE. For grading purpose, let's use the `:LD_MMA` ([Method of Moving Asymptotes](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes-and-ccsa)) algorithm in NLopt.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.356237077697068e6\n",
      "\n",
      "  0.967089 seconds (1.10 M allocations: 72.903 MiB, 85.16% compilation time)\n",
      "objective value at solution: -2.840058786650191e6)\n",
      "solution values:\n",
      "lmm.β = [0.1814776101361502, 6.50038355413275, -3.4998642895680585, 0.9997119258286763, 4.9992294818701675]\n",
      "lmm.σ² = [1.4987345756953059]\n",
      "lmm.L * transpose(lmm.L) = [1.9836266207662998 0.06578009077946281 0.055171453028808964; 0.06578009077946281 1.28144118940443 -0.09059652736484758; 0.055171453028808964 -0.09059652736484758 0.943427729674199]\n",
      "gradient @ solution:\n",
      "lmm.∇β = [0.020715013384039804, -0.007735285610219078, -0.007759917004352523, -0.0006757573451210419, -0.0007069367911913815]\n",
      "lmm.∇σ² = [-0.00921637415905252]\n",
      "lmm.∇L = [-0.00016051920270561057 -0.002991258888290063 0.053537668111773296; 0.0019772269599108577 0.00021146130675053776 0.0013620245882379686; 0.07846763773772997 0.0006943460169284016 0.010634576249044514]\n",
      "norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²]) = 0.08312512919941961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08312512919941961"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "# NLopt (LD_MMA) obj. val = -2.8400587866501966e6\n",
    "NLopt_solver = NLopt.Optimizer()\n",
    "MOI.set(NLopt_solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "@time fit!(lmm, NLopt_solver)\n",
    "\n",
    "println(\"objective value at solution: $(logl!(lmm)))\")\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L\n",
    "@show norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "You get 10 points if the following code does not throw `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.840059e6\n",
    "# gradient at solution should be small enough\n",
    "@assert norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²]) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "My median run time is 50ms. You get 10 points if your median time is within 1s(=1000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 36 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m137.613 ms\u001b[22m\u001b[39m … \u001b[35m192.441 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m137.780 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m140.538 ms\u001b[22m\u001b[39m ± \u001b[32m 10.622 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[34m█\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m \u001b[39m▁\n",
       "  138 ms\u001b[90m           Histogram: frequency by time\u001b[39m          192 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m18.16 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m523\u001b[39m."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLopt_solver = NLopt.Optimizer()\n",
    "MOI.set(NLopt_solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "bm_mma = @benchmark fit!($lmm, $(NLopt_solver)) setup=(init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_mma).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. (10 pts) Gradient free vs gradient-based methods\n",
    "\n",
    "Advantage of using a modelling tool such as MathOptInterface.jl is that we can easily switch the backend solvers. For a research problem, we never know beforehand which solver works best. \n",
    "\n",
    "Try different solvers in the NLopt.jl and Ipopt.jl packages. Compare the results in terms of run times (the shorter the better), objective values at solution (the larger the better), and gradients at solution (closer to 0 the better). Summarize what you find.\n",
    "\n",
    "See this [page](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/) for the descriptions of algorithms in NLopt.\n",
    "\n",
    "Documentation for the Ipopt can be found [here](https://coin-or.github.io/Ipopt/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\"NLopt (LN_COBYLA, gradient free)\", \"NLopt (LD_MMA, gradient-based)\", \n",
    "    \"Ipopt (L-BFGS)\"]\n",
    "\n",
    "function setup_solver(s::String)\n",
    "    if s == \"NLopt (LN_COBYLA, gradient free)\"\n",
    "        solver = NLopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LN_COBYLA)\n",
    "    elseif s == \"NLopt (LD_MMA, gradient-based)\"\n",
    "        solver = NLopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "    elseif s == \"Ipopt (L-BFGS)\"\n",
    "        solver = Ipopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"print_level\"), 0)\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"hessian_approximation\"), \"limited-memory\")\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"tol\"), 1e-6)\n",
    "    elseif s == \"Ipopt (use FIM)\"\n",
    "        # Ipopt (use Hessian) obj val = -2.8400587866468e6\n",
    "        solver = Ipopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"print_level\"), 0)        \n",
    "    else\n",
    "        error(\"unrecognized solver $s\")\n",
    "    end\n",
    "    solver\n",
    "end\n",
    "\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for i in 1:length(solvers)\n",
    "    solver = setup_solver(solvers[i])\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────┬─────────┬───────────────────┬───────────────┐\n",
      "│\u001b[1m                           Solver \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m          Log-Like \u001b[0m│\u001b[1m Gradiant Norm \u001b[0m│\n",
      "├──────────────────────────────────┼─────────┼───────────────────┼───────────────┤\n",
      "│ NLopt (LN_COBYLA, gradient free) │    0.54 │ -2840081.23944276 │  952.86875390 │\n",
      "│   NLopt (LD_MMA, gradient-based) │    0.14 │ -2840058.78665019 │    0.08312513 │\n",
      "│                   Ipopt (L-BFGS) │    4.03 │ -2840058.78664680 │    0.00113608 │\n",
      "└──────────────────────────────────┴─────────┴───────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# display results\n",
    "pretty_table(\n",
    "    hcat(solvers, runtime, objvals, gradnrm),\n",
    "    header = [\"Solver\", \"Runtime\", \"Log-Like\", \"Gradiant Norm\"],\n",
    "    formatters = (ft_printf(\"%5.2f\", 2), ft_printf(\"%8.8f\", 3:4))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Although `L-BFGS` achieves the largest objective value and the smallest gradients, `LD_MMA` appears to be the best choice: It is more than ten times faster than `L-BFGS` while maintaining almost the same objective value and a gradient norm that is sufficiently close to zero. `LN_COBYLA`(gradient-free) is likely the worst option due to its gradient norm being far from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. (10 pts) Compare with existing art\n",
    "\n",
    "Let's compare our method with lme4 package in R and MixedModels.jl package in Julia. Both lme4 and MixedModels.jl are developed mainly by Doug Bates. Summarize what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"My method\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.840058786650191e6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = setup_solver(\"NLopt (LD_MMA, gradient-based)\")\n",
    "bm_257 = @benchmark fit!($lmm, $solver) setup=(init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lme4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mRCall.jl: 要求されたパッケージ Matrix をロード中です\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ RCall ~/.julia/packages/RCall/dDAVd/src/io.jl:172\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mRCall.jl: Rows: 1744977 Columns: 8\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m── Column specification ────────────────────────────────────────────────────────\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mDelimiter: \",\"\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mdbl (8): ID, Y, X1, X2, X3, X4, Z1, Z2\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ RCall ~/.julia/packages/RCall/dDAVd/src/io.jl:172\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{VecSxp}\n",
       "# A tibble: 1,744,977 × 8\n",
       "      ID        Y     X1      X2     X3      X4      Z1      Z2\n",
       "   <dbl>    <dbl>  <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n",
       " 1     1   9.52    0.202 -0.463   0.798  0.734   0.685  -0.570 \n",
       " 2     1  24.4     1.59  -1.95    1.20   1.43    1.64    0.369 \n",
       " 3     1  -1.99    0.378 -0.0367  1.63  -1.15   -0.818   2.83  \n",
       " 4     1 -17.4    -1.88   0.375  -0.498 -0.253   1.56    1.68  \n",
       " 5     1  -0.0704  0.658 -0.165   0.780 -1.23   -0.0288 -1.09  \n",
       " 6     1  -0.853   0.458 -0.313  -0.512 -0.800  -0.331   1.98  \n",
       " 7     1  -1.80    0.220  0.328   1.32  -1.01   -0.363  -0.0703\n",
       " 8     1   5.88    1.30   0.889  -0.854  0.0714 -0.658  -0.0339\n",
       " 9     1  -9.21   -1.43  -0.522  -0.119 -0.580  -0.155  -1.89  \n",
       "10     1 -11.3    -0.468 -0.700   0.872 -1.82    1.80    0.492 \n",
       "# ℹ 1,744,967 more rows\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mRCall.jl: checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,  で警告がありました:\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  Model failed to converge with max|grad| = 0.00245559 (tol = 0.002, component 1)\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ RCall ~/.julia/packages/RCall/dDAVd/src/io.jl:172\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{RealSxp}\n",
       "  ユーザ システム     経過 \n",
       " 107.062    8.083  116.033 \n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MixedModels.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1744977×8 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1744952 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">ID</th><th style = \"text-align: left;\">Y</th><th style = \"text-align: left;\">X1</th><th style = \"text-align: left;\">X2</th><th style = \"text-align: left;\">X3</th><th style = \"text-align: left;\">X4</th><th style = \"text-align: left;\">Z1</th><th style = \"text-align: left;\">Z2</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">9.52102</td><td style = \"text-align: right;\">0.201821</td><td style = \"text-align: right;\">-0.463234</td><td style = \"text-align: right;\">0.797731</td><td style = \"text-align: right;\">0.73357</td><td style = \"text-align: right;\">0.685476</td><td style = \"text-align: right;\">-0.569622</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">24.4063</td><td style = \"text-align: right;\">1.58557</td><td style = \"text-align: right;\">-1.94608</td><td style = \"text-align: right;\">1.19787</td><td style = \"text-align: right;\">1.43149</td><td style = \"text-align: right;\">1.63962</td><td style = \"text-align: right;\">0.369053</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-1.99215</td><td style = \"text-align: right;\">0.378332</td><td style = \"text-align: right;\">-0.0367002</td><td style = \"text-align: right;\">1.63072</td><td style = \"text-align: right;\">-1.15031</td><td style = \"text-align: right;\">-0.817843</td><td style = \"text-align: right;\">2.83422</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-17.4233</td><td style = \"text-align: right;\">-1.8826</td><td style = \"text-align: right;\">0.374561</td><td style = \"text-align: right;\">-0.49786</td><td style = \"text-align: right;\">-0.253248</td><td style = \"text-align: right;\">1.56433</td><td style = \"text-align: right;\">1.67857</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.0704245</td><td style = \"text-align: right;\">0.658283</td><td style = \"text-align: right;\">-0.165487</td><td style = \"text-align: right;\">0.77951</td><td style = \"text-align: right;\">-1.22763</td><td style = \"text-align: right;\">-0.0287779</td><td style = \"text-align: right;\">-1.09172</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.853357</td><td style = \"text-align: right;\">0.457784</td><td style = \"text-align: right;\">-0.313387</td><td style = \"text-align: right;\">-0.512299</td><td style = \"text-align: right;\">-0.800278</td><td style = \"text-align: right;\">-0.330632</td><td style = \"text-align: right;\">1.97609</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-1.80061</td><td style = \"text-align: right;\">0.220461</td><td style = \"text-align: right;\">0.327879</td><td style = \"text-align: right;\">1.32209</td><td style = \"text-align: right;\">-1.01336</td><td style = \"text-align: right;\">-0.362947</td><td style = \"text-align: right;\">-0.0703055</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">5.88119</td><td style = \"text-align: right;\">1.30135</td><td style = \"text-align: right;\">0.88884</td><td style = \"text-align: right;\">-0.853941</td><td style = \"text-align: right;\">0.0714372</td><td style = \"text-align: right;\">-0.658202</td><td style = \"text-align: right;\">-0.0338648</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-9.20504</td><td style = \"text-align: right;\">-1.43248</td><td style = \"text-align: right;\">-0.521638</td><td style = \"text-align: right;\">-0.119287</td><td style = \"text-align: right;\">-0.579596</td><td style = \"text-align: right;\">-0.154869</td><td style = \"text-align: right;\">-1.88707</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-11.2909</td><td style = \"text-align: right;\">-0.46827</td><td style = \"text-align: right;\">-0.699709</td><td style = \"text-align: right;\">0.871668</td><td style = \"text-align: right;\">-1.81529</td><td style = \"text-align: right;\">1.79726</td><td style = \"text-align: right;\">0.492339</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">8.9367</td><td style = \"text-align: right;\">2.00212</td><td style = \"text-align: right;\">1.1577</td><td style = \"text-align: right;\">-0.973746</td><td style = \"text-align: right;\">-0.784991</td><td style = \"text-align: right;\">-2.12648</td><td style = \"text-align: right;\">-1.68961</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.496904</td><td style = \"text-align: right;\">-0.480658</td><td style = \"text-align: right;\">0.1732</td><td style = \"text-align: right;\">-1.27733</td><td style = \"text-align: right;\">0.571911</td><td style = \"text-align: right;\">-0.802521</td><td style = \"text-align: right;\">0.584438</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.139879</td><td style = \"text-align: right;\">0.597359</td><td style = \"text-align: right;\">-0.894785</td><td style = \"text-align: right;\">-0.904349</td><td style = \"text-align: right;\">-1.10844</td><td style = \"text-align: right;\">0.106724</td><td style = \"text-align: right;\">-0.5961</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744966</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">14.3459</td><td style = \"text-align: right;\">2.53537</td><td style = \"text-align: right;\">-0.385281</td><td style = \"text-align: right;\">0.374004</td><td style = \"text-align: right;\">-1.06646</td><td style = \"text-align: right;\">-0.881868</td><td style = \"text-align: right;\">-0.524693</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744967</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-1.86731</td><td style = \"text-align: right;\">-0.156314</td><td style = \"text-align: right;\">-0.359602</td><td style = \"text-align: right;\">-1.00392</td><td style = \"text-align: right;\">-0.463548</td><td style = \"text-align: right;\">-1.40846</td><td style = \"text-align: right;\">0.289986</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744968</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-3.12427</td><td style = \"text-align: right;\">-0.393863</td><td style = \"text-align: right;\">-0.491481</td><td style = \"text-align: right;\">1.55365</td><td style = \"text-align: right;\">-0.776784</td><td style = \"text-align: right;\">0.688556</td><td style = \"text-align: right;\">-1.16616</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744969</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-3.20226</td><td style = \"text-align: right;\">-0.126944</td><td style = \"text-align: right;\">0.601992</td><td style = \"text-align: right;\">-0.960992</td><td style = \"text-align: right;\">-0.300833</td><td style = \"text-align: right;\">1.07374</td><td style = \"text-align: right;\">0.649807</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744970</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">0.0979076</td><td style = \"text-align: right;\">-0.0942822</td><td style = \"text-align: right;\">-0.017406</td><td style = \"text-align: right;\">1.01639</td><td style = \"text-align: right;\">-0.111906</td><td style = \"text-align: right;\">-0.708675</td><td style = \"text-align: right;\">0.0446113</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744971</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">6.84005</td><td style = \"text-align: right;\">0.286095</td><td style = \"text-align: right;\">-0.995131</td><td style = \"text-align: right;\">-0.293129</td><td style = \"text-align: right;\">0.193649</td><td style = \"text-align: right;\">-1.03079</td><td style = \"text-align: right;\">-1.9487</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744972</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">8.44826</td><td style = \"text-align: right;\">-0.0458988</td><td style = \"text-align: right;\">-0.00257329</td><td style = \"text-align: right;\">-1.31491</td><td style = \"text-align: right;\">1.51216</td><td style = \"text-align: right;\">0.0174882</td><td style = \"text-align: right;\">0.530319</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744973</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-5.13234</td><td style = \"text-align: right;\">-1.11932</td><td style = \"text-align: right;\">-1.06951</td><td style = \"text-align: right;\">-0.339256</td><td style = \"text-align: right;\">-0.500339</td><td style = \"text-align: right;\">0.737606</td><td style = \"text-align: right;\">1.58472</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744974</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-0.530812</td><td style = \"text-align: right;\">-0.850859</td><td style = \"text-align: right;\">0.00500998</td><td style = \"text-align: right;\">-0.274531</td><td style = \"text-align: right;\">1.01461</td><td style = \"text-align: right;\">0.382234</td><td style = \"text-align: right;\">-0.435956</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744975</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-8.88652</td><td style = \"text-align: right;\">-0.762326</td><td style = \"text-align: right;\">0.154867</td><td style = \"text-align: right;\">-0.933923</td><td style = \"text-align: right;\">-0.931906</td><td style = \"text-align: right;\">0.797417</td><td style = \"text-align: right;\">0.888702</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744976</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-1.42159</td><td style = \"text-align: right;\">-0.672599</td><td style = \"text-align: right;\">-1.25194</td><td style = \"text-align: right;\">-0.518835</td><td style = \"text-align: right;\">-0.0647723</td><td style = \"text-align: right;\">-1.48993</td><td style = \"text-align: right;\">-0.00689683</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744977</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">3.79889</td><td style = \"text-align: right;\">-1.21758</td><td style = \"text-align: right;\">-1.49033</td><td style = \"text-align: right;\">1.04599</td><td style = \"text-align: right;\">0.459857</td><td style = \"text-align: right;\">0.628323</td><td style = \"text-align: right;\">0.856971</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& ID & Y & X1 & X2 & X3 & X4 & Z1 & Z2\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 9.52102 & 0.201821 & -0.463234 & 0.797731 & 0.73357 & 0.685476 & -0.569622 \\\\\n",
       "\t2 & 1 & 24.4063 & 1.58557 & -1.94608 & 1.19787 & 1.43149 & 1.63962 & 0.369053 \\\\\n",
       "\t3 & 1 & -1.99215 & 0.378332 & -0.0367002 & 1.63072 & -1.15031 & -0.817843 & 2.83422 \\\\\n",
       "\t4 & 1 & -17.4233 & -1.8826 & 0.374561 & -0.49786 & -0.253248 & 1.56433 & 1.67857 \\\\\n",
       "\t5 & 1 & -0.0704245 & 0.658283 & -0.165487 & 0.77951 & -1.22763 & -0.0287779 & -1.09172 \\\\\n",
       "\t6 & 1 & -0.853357 & 0.457784 & -0.313387 & -0.512299 & -0.800278 & -0.330632 & 1.97609 \\\\\n",
       "\t7 & 1 & -1.80061 & 0.220461 & 0.327879 & 1.32209 & -1.01336 & -0.362947 & -0.0703055 \\\\\n",
       "\t8 & 1 & 5.88119 & 1.30135 & 0.88884 & -0.853941 & 0.0714372 & -0.658202 & -0.0338648 \\\\\n",
       "\t9 & 1 & -9.20504 & -1.43248 & -0.521638 & -0.119287 & -0.579596 & -0.154869 & -1.88707 \\\\\n",
       "\t10 & 1 & -11.2909 & -0.46827 & -0.699709 & 0.871668 & -1.81529 & 1.79726 & 0.492339 \\\\\n",
       "\t11 & 1 & 8.9367 & 2.00212 & 1.1577 & -0.973746 & -0.784991 & -2.12648 & -1.68961 \\\\\n",
       "\t12 & 1 & -0.496904 & -0.480658 & 0.1732 & -1.27733 & 0.571911 & -0.802521 & 0.584438 \\\\\n",
       "\t13 & 1 & -0.139879 & 0.597359 & -0.894785 & -0.904349 & -1.10844 & 0.106724 & -0.5961 \\\\\n",
       "\t14 & 1 & -8.71947 & -0.511146 & -1.338 & -1.18855 & -1.8141 & 0.469028 & -0.172564 \\\\\n",
       "\t15 & 1 & -2.58409 & -0.597996 & 0.059796 & -0.655992 & 0.0857626 & -1.16351 & -0.240554 \\\\\n",
       "\t16 & 1 & -5.81281 & 0.411499 & 0.437668 & -0.725361 & -0.812424 & 1.99023 & -0.768594 \\\\\n",
       "\t17 & 1 & -0.596476 & 0.0463844 & 0.0538646 & 0.663663 & 0.51955 & 2.30564 & 0.4257 \\\\\n",
       "\t18 & 1 & -3.18641 & -1.49591 & 0.659436 & 0.956284 & 1.24877 & -0.824118 & 0.177151 \\\\\n",
       "\t19 & 1 & 1.56221 & -0.606838 & 1.08345 & 0.163253 & 1.84409 & -0.511557 & -0.994212 \\\\\n",
       "\t20 & 1 & 3.02138 & 0.0405701 & 0.0647205 & 1.22975 & -0.0210971 & -1.50206 & -0.414032 \\\\\n",
       "\t21 & 1 & 22.7553 & 2.22447 & 0.917703 & 0.621901 & 2.02733 & -0.0768099 & -0.0520619 \\\\\n",
       "\t22 & 1 & 3.70676 & 0.56003 & -1.49019 & 0.368436 & -1.24426 & -0.853134 & 0.712161 \\\\\n",
       "\t23 & 1 & -8.04279 & -0.823574 & -0.627208 & -0.956862 & -1.0348 & -0.873069 & -0.856131 \\\\\n",
       "\t24 & 1 & -2.33705 & -0.721538 & 0.362801 & 0.548455 & 0.894375 & 1.89845 & 1.32574 \\\\\n",
       "\t25 & 1 & 14.8916 & 0.626803 & -2.96424 & -0.747008 & 0.0964266 & 0.117514 & 1.64184 \\\\\n",
       "\t26 & 1 & 18.3082 & 0.770025 & -0.257942 & 0.513041 & 2.25855 & 0.571801 & -1.39796 \\\\\n",
       "\t27 & 1 & 1.19626 & 0.0607296 & 0.315506 & -0.279184 & 0.42981 & -0.546869 & 0.118436 \\\\\n",
       "\t28 & 1 & -15.2916 & -0.830792 & 2.3629 & -0.210138 & -0.659754 & -0.717479 & 0.223425 \\\\\n",
       "\t29 & 1 & 8.53769 & 1.02613 & -0.0021617 & -1.35241 & -0.0927837 & -0.666525 & -0.0567915 \\\\\n",
       "\t30 & 1 & -10.3919 & -0.465915 & 0.560413 & -1.41106 & -0.921584 & -1.17228 & -0.219102 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1744977×8 DataFrame\u001b[0m\n",
       "\u001b[1m     Row \u001b[0m│\u001b[1m ID     \u001b[0m\u001b[1m Y           \u001b[0m\u001b[1m X1         \u001b[0m\u001b[1m X2          \u001b[0m\u001b[1m X3        \u001b[0m\u001b[1m X4        \u001b[0m ⋯\n",
       "         │\u001b[90m String \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m ⋯\n",
       "─────────┼──────────────────────────────────────────────────────────────────────\n",
       "       1 │ 1         9.52102     0.201821   -0.463234     0.797731   0.73357   ⋯\n",
       "       2 │ 1        24.4063      1.58557    -1.94608      1.19787    1.43149\n",
       "       3 │ 1        -1.99215     0.378332   -0.0367002    1.63072   -1.15031\n",
       "       4 │ 1       -17.4233     -1.8826      0.374561    -0.49786   -0.253248\n",
       "       5 │ 1        -0.0704245   0.658283   -0.165487     0.77951   -1.22763   ⋯\n",
       "       6 │ 1        -0.853357    0.457784   -0.313387    -0.512299  -0.800278\n",
       "       7 │ 1        -1.80061     0.220461    0.327879     1.32209   -1.01336\n",
       "       8 │ 1         5.88119     1.30135     0.88884     -0.853941   0.0714372\n",
       "       9 │ 1        -9.20504    -1.43248    -0.521638    -0.119287  -0.579596  ⋯\n",
       "      10 │ 1       -11.2909     -0.46827    -0.699709     0.871668  -1.81529\n",
       "      11 │ 1         8.9367      2.00212     1.1577      -0.973746  -0.784991\n",
       "    ⋮    │   ⋮          ⋮           ⋮            ⋮           ⋮          ⋮      ⋱\n",
       " 1744968 │ 1000     -3.12427    -0.393863   -0.491481     1.55365   -0.776784\n",
       " 1744969 │ 1000     -3.20226    -0.126944    0.601992    -0.960992  -0.300833  ⋯\n",
       " 1744970 │ 1000      0.0979076  -0.0942822  -0.017406     1.01639   -0.111906\n",
       " 1744971 │ 1000      6.84005     0.286095   -0.995131    -0.293129   0.193649\n",
       " 1744972 │ 1000      8.44826    -0.0458988  -0.00257329  -1.31491    1.51216\n",
       " 1744973 │ 1000     -5.13234    -1.11932    -1.06951     -0.339256  -0.500339  ⋯\n",
       " 1744974 │ 1000     -0.530812   -0.850859    0.00500998  -0.274531   1.01461\n",
       " 1744975 │ 1000     -8.88652    -0.762326    0.154867    -0.933923  -0.931906\n",
       " 1744976 │ 1000     -1.42159    -0.672599   -1.25194     -0.518835  -0.0647723\n",
       " 1744977 │ 1000      3.79889    -1.21758    -1.49033      1.04599    0.459857  ⋯\n",
       "\u001b[36m                                              2 columns and 1744956 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = CSV.File(\"lmm_data.csv\", types = Dict(1=>String)) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\u001b[39m\n",
      "\r",
      "\u001b[32mMinimizing 192    Time: 0:00:00 ( 1.99 ms/it)\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.756524625"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), $testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 7 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m686.260 ms\u001b[22m\u001b[39m … \u001b[35m   1.274 s\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m1.88% … 24.70%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m756.525 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m4.19%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m855.248 ms\u001b[22m\u001b[39m ± \u001b[32m212.425 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m8.46% ±  8.00%\n",
       "\n",
       "  \u001b[39m█\u001b[39m \u001b[39m█\u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  686 ms\u001b[90m           Histogram: frequency by time\u001b[39m          1.27 s \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m826.57 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m7850\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{tabular}\n",
       "{l | r | r | r | r | r}\n",
       " & Est. & SE & z & p & $\\sigma_\\text{ID}$ \\\\\n",
       "\\hline\n",
       "(Intercept) & 0.1815 & 0.0444 & 4.08 & <1e-04 & 1.4079 \\\\\n",
       "X1 & 6.5004 & 0.0009 & 7000.80 & <1e-99 &   \\\\\n",
       "X2 & -3.4999 & 0.0009 & -3768.92 & <1e-99 &   \\\\\n",
       "X3 & 0.9997 & 0.0009 & 1078.16 & <1e-99 &   \\\\\n",
       "X4 & 4.9992 & 0.0009 & 5391.08 & <1e-99 &   \\\\\n",
       "Z2 &  &  &  &  & 0.9712 \\\\\n",
       "Z1 &  &  &  &  & 1.1319 \\\\\n",
       "Residual & 1.2242 &  &  &  &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "|             |    Est. |     SE |        z |      p |   σ_ID |\n",
       "|:----------- | -------:| ------:| --------:| ------:| ------:|\n",
       "| (Intercept) |  0.1815 | 0.0444 |     4.08 | <1e-04 | 1.4079 |\n",
       "| X1          |  6.5004 | 0.0009 |  7000.80 | <1e-99 |        |\n",
       "| X2          | -3.4999 | 0.0009 | -3768.92 | <1e-99 |        |\n",
       "| X3          |  0.9997 | 0.0009 |  1078.16 | <1e-99 |        |\n",
       "| X4          |  4.9992 | 0.0009 |  5391.08 | <1e-99 |        |\n",
       "| Z2          |         |        |          |        | 0.9712 |\n",
       "| Z1          |         |        |          |        | 1.1319 |\n",
       "| Residual    |  1.2242 |        |          |        |        |\n"
      ],
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " Y ~ 1 + X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)\n",
       "     logLik     -2 logLik        AIC           AICc          BIC      \n",
       " -2840058.7869  5680117.5739  5680141.5739  5680141.5740  5680290.0409\n",
       "\n",
       "Variance components:\n",
       "            Column   Variance Std.Dev.   Corr.\n",
       "ID       (Intercept)  1.982128 1.407881\n",
       "         Z1           1.281196 1.131899 +0.04\n",
       "         Z2           0.943221 0.971196 +0.04 -0.08\n",
       "Residual              1.498736 1.224229\n",
       " Number of obs: 1744977; levels of grouping factors: 1000\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────\n",
       "                 Coef.   Std. Error         z  Pr(>|z|)\n",
       "───────────────────────────────────────────────────────\n",
       "(Intercept)   0.181524  0.0444498        4.08    <1e-04\n",
       "X1            6.50038   0.00092852    7000.80    <1e-99\n",
       "X2           -3.49986   0.000928612  -3768.92    <1e-99\n",
       "X3            0.999712  0.000927237   1078.16    <1e-99\n",
       "X4            4.99923   0.000927316   5391.08    <1e-99\n",
       "───────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────┬─────────┬─────────────────┐\n",
      "│\u001b[1m         Method \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m        Log-Like \u001b[0m│\n",
      "├────────────────┼─────────┼─────────────────┤\n",
      "│      My method │    0.14 │ -2840058.786650 │\n",
      "│           lme4 │  116.03 │ -2840058.786652 │\n",
      "│ MixedModels.jl │    0.76 │ -2840058.786933 │\n",
      "└────────────────┴─────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "pretty_table(\n",
    "    hcat(method, runtime, loglike),\n",
    "    header = [\"Method\", \"Runtime\", \"Log-Like\"],\n",
    "    formatters = (ft_printf(\"%5.2f\", 2), ft_printf(\"%8.6f\", 3))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "My method achieved a runtime that is 100 times faster than `lme4` and 5-10 times faster than `MixedModels.jl`, while maintaining the same objective value as these existing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Be proud of yourself\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analysis on complex longitudinal data sets with millions of records. And you beat current software by XXX fold."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
